---
# IMPORTANT: Change settings here, but DO NOT change the spacing.
# Remove comments and add values where applicable.
# The descriptions below should be self-explanatory

title: "Helping You Write Academic Papers in R using Texevier"
#subtitle: "This will appear as Right Header"

documentclass: "elsarticle"

# --------- Thesis title (Optional - set to FALSE by default).
# You can move the details below around as you please.
Thesis_FP: FALSE
# Entry1: "An unbelievable study with a title spanning multiple lines."
# Entry2: "\\textbf{Nico Katzke}" # textbf for bold
# Entry3: "A thesis submitted toward the degree of Doctor of Philosophy"
# Uni_Logo: Tex/Logo.png # Place a  png logo in an img folder in your root and uncomment this. Leave uncommented for no image
# Logo_width: 0.3 # If using a logo - use this to set width (size) of image
# Entry4: "Under the supervision of: \\vfill Prof. Joe Smith and Dr. Frank Smith"
# Entry5: "Stellenbosch University"
# Entry6: April 2020
# Entry7:
# Entry8:

# --------- Front Page
# Comment: ----- Follow this pattern for up to 5 authors
AddTitle: TRUE # Use FALSE when submitting to peer reviewed platform. This will remove author names.
Author1: "Nico Katzke^[__Contributions:__  \\newline _The authors would like to thank no institution for money donated to this project. Thank you sincerely._]"  # First Author - note the thanks message displayed as an italic footnote of first page.
Ref1: "Prescient Securities, Cape Town, South Africa" # First Author's Affiliation
Email1: "nfkatzke\\@gmail.com" # First Author's Email address

Author2: "John Smith"
Ref2: "Some other Institution, Cape Town, South Africa"
Email2: "John\\@gmail.com"
CommonAffiliation_12: TRUE # If Author 1 and 2 have a common affiliation. Works with _13, _23, etc.

Author3: "John Doe"
Email3: "Joe\\@gmail.com"

CorrespAuthor_1: TRUE  # If corresponding author is author 3, e.g., use CorrespAuthor_3: TRUE

keywords: "Multivariate GARCH \\sep Kalman Filter \\sep Copula" # Use \\sep to separate
JELCodes: "L250 \\sep L100"

# ----- Manage headers and footers:
#BottomLFooter: $Title$
#BottomCFooter:
#TopLHeader: \leftmark # Adds section name at topleft. Remove comment to add it.
BottomRFooter: "\\footnotesize Page \\thepage" # Add a '#' before this line to remove footer.
addtoprule: TRUE
addfootrule: TRUE               # Use if footers added. Add '#' to remove line.

# --------- page margins:
margin: 2.3 # Sides
bottom: 2 # bottom
top: 2.5 # Top
HardSet_layout: TRUE # Hard-set the spacing of words in your document. This will stop LaTeX squashing text to fit on pages, e.g.
# This is done by hard-setting the spacing dimensions. Set to FALSE if you want LaTeX to optimize this for your paper.

# --------- Line numbers
linenumbers: FALSE # Used when submitting to journal

# ---------- References settings:
# You can download cls format here: https://www.zotero.org/ - simply search for your institution. You can also edit and save cls formats here: https://editor.citationstyles.org/about/
# Hit download, store it in Tex/ folder, and change reference below - easy.
bibliography: Tex/ref.bib       # Do not edit: Keep this naming convention and location.
csl: Tex/harvard-stellenbosch-university.csl # referencing format used.
# By default, the bibliography only displays the cited references. If you want to change this, you can comment out one of the following:
#nocite: '@*' # Add all items in bibliography, whether cited or not
# nocite: |  # add specific references that aren't cited
#  @grinold2000
#  @Someoneelse2010

# ---------- General:
RemovePreprintSubmittedTo: TRUE  # Removes the 'preprint submitted to...' at bottom of titlepage
Journal: "Journal of Finance"   # Journal that the paper will be submitting to, if RemovePreprintSubmittedTo is set to TRUE.
toc: FALSE                       # Add a table of contents
numbersections: TRUE             # Should sections (and thus figures and tables) be numbered?
fontsize: 11pt                  # Set fontsize
linestretch: 1.2                # Set distance between lines.
link-citations: TRUE            # This creates dynamic links to the papers in reference list.

### Adding additional latex packages:
header-includes:
    - \usepackage{amsmath} # Add additional packages here.

output:
  pdf_document:
    keep_tex: TRUE
    template: Tex/TexDefault.txt
    fig_width: 3.5 # Adjust default figure sizes. This can also be done in the chunks of the text.
    fig_height: 3.5
abstract: |
  Abstract to be written here. The abstract should not be too long and should provide the reader with a good understanding what you are writing about. Academic papers are not like novels where you keep the reader in suspense. To be effective in getting others to read your paper, be as open and concise about your findings here as possible. Ideally, upon reading your abstract, the reader should feel he / she must read your paper in entirety.
---

<!-- First: Set your default preferences for chunk options: -->

<!-- If you want a chunk's code to be printed, set echo = TRUE. message = FALSE stops R printing ugly package loading details in your final paper too. I also suggest setting warning = FALSE and checking for warnings in R, else you might find ugly warnings in your paper. -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 6, fig.height = 5, fig.pos="H", fig.pos = 'H')
# Note: Include = FALSE implies the code is executed, but not printed in your pdf.
# warning and message = FALSE implies ugly messages and warnings are removed from your pdf.
# These should be picked up when you execute the command chunks (code sections below) in your rmd, not printed in your paper!

# Lets load in example data, and see how this can be stored and later called from your 'data' folder.
if(!require("tidyverse")) install.packages("tidyverse")
library(tidyverse)
Example_data <- Texevier::Ex_Dat

# Notice that as you are working in a .Rproj file (I am assuming you are) - the relative paths of your directories start at your specified root.
# This means that when working in a .Rproj file, you never need to use getwd() - it is assumed as your base root automatically.
write_rds(Example_data, path = "data/Example_data.rds")

```


<!-- ############################## -->
<!-- # Start Writing here: -->
<!-- ############################## -->

# Introduction \label{Introduction}

Since Harry Markovitz's (1952) seminal work on mean-variance portfolios scholars from around the globe have been aspiring to develop a robust algorithm capable of situating a portfolio on the efficient frontier _ex ante_. There are now a wide array of available alternatives portfolio optimisers raging from simple heuristic based approaches to advanced mathematical algorithms based on quadratic optimization, random matrix theory and machine learning methods; with many more are still in the making.

Unfortunately, portfolio optimisers of the mean-variance type suffer from sensitivity to changes in their expected return forecast, which are naturally extremely difficult to forecast [@lopez]. Due to this concern, this work only evaluates the risk-based portfolio strategies, defined by @leote as "systemic quantitative approaches to portfolio allocation" that solely rely on views of risk when allocating capital. These strategies therefore, do not require expected return forecasts. 

importance of Monte Carlo methods

path dependence; first rule of investment management

Due to the aforementioned sensitivity issues surrounding errors in the expected return estimation, this work will only cover so-called risk based portfolios, since these techniques intentionally forego this input. These include the naive equal weight, inverse variance, hierarchical risk parity, equal risk contribution and the minimum variance portfolios. _The theoretical underpinnings of each will be reviewed as well as their relative performance in historical back tests_.



# Aims and Objectives

This work aims to use Monte Carlo Methods to uncover the relationship between a market's covariance structure and the risk return properties of various risk based portfolio algorithms. This will be achieved through the following objectives.

1. Design and create four distinctive \emph{ad hoc} correlation matrices and build one empirical _50 by 50_ correlation matrix, each representing a market with a different risk structure. These from markets structure possessing no clusters to those exhibiting hierarchical clustering.

2. Use the R package _MCmarket_ to perform Monte Carlo Simulations, using each of the five correlation matrices from step one as the primary input [REFERENCE MYSELF]. The markets will be built to posses student t multivariate distributions, with 3 degrees of freedom. Meanwhile the individual asset returns will each be normally distributed with a random mean and standard deviation. Each market type will be simulated 10 000 times across 300 periods. 

3. Use the simulated market data to calculate the returns obtained from various risk based portfolio's. The first __100__ periods will be used estimate an out of sample covariance matrix, this will be used to calculate portfolio weights. These weights will remain for the next 50 periods after which portfolios will be rebalanced by looking back _100_ periods, recalculating the covariance matrix and the new portfolio weights. This process is repeated until all _300_ simulated periods have been considered. Therefore, each portfolio will end up with a series of _199_ returns.

4. The performance of each portfolio will then be compared and contrasted using various portfolio risk/return analytics. Portfolio optimisers will be compared with each other within market types and with themselves across markets types. 


# Litrature Review

## A Review of Portfolio Optimisation Algorithms

### Introduction

Since Harry Markovitz's (1952) seminal work on mean-variance portfolios scholars from around the globe have been aspiring to develop a robust algorithm capable of situating a portfolio on the efficient frontier _ex ante_. There are now a wide array of available alternatives portfolio optimisers raging from simple heuristic based approaches to advanced mathematical algorithms based on quadratic optimization, random matrix theory and machine learning methods; with many more are still in the making.

This literature review review will cover some common issues discussed within the literature surrounding portfolio optimization in general, the five risk-based portfolios evaluated in this work, their respective performance in empirical back tests and Monte Carlo Studies and finally the importance of using Monte Carlo methods when stress testing portfolio optimization algorithms will be discussed. 

### Common Issues Portfolio Optimizers
 
When working with in sample data Portfolio optimization tends to be a perfect science, but out of sample it becomes more of an art form where it is often preferable to work on heuristic than hard rules. This section highlights some gneral issues, highlighted within the portfolio optimization literature, that tend to worsen their out of sample performance.

Firstly, mean-variance optimisers, like those introduced by @markowitz, rely heavily on the accuracy of their expected return forecasts. Small changes in the expected return input can lead to large changes in portfolio weights [@lopez]. Since in practice expected returns are extremely difficult, if not impossible, to accurately estimate, this issue serves as a major hindrance to their wide spread use. Due to this issue the so-called risk based portfolio's that intentionally avoid using expected return forecasts have garnered a lot of attention [@maillard2010]. 

Unfortunately, these risk based portfolios are not void of issues. The quadratic programming methods used in many portfolio optimisers, including the mean variance and many risk-based, require the inversion of some positive-definite covariance matrix. This positive definiteness requirement can cause issues as covariance matrices estimated on empirical data are sometimes not positive definite, in which case their inverse does not exist and these portfolio's don't have solutions [@ref]. A common method to get around this issue is to simply transform the non-posetive definate matrix into its closest positive definite form. 

The covariance estimation step is particularly susceptible to error if the covariance matrix suffers from a high condition number. A condition number is defined as the absolute value of the ratio between a covariance matrix's largest and smallest eigenvalues [@lopez; @lopez2012]. The condition number is smallest in diagonal matrices (they have a condition number of 1) and it increases as more correlated variables are added. When working with high condition number matrices a small change in a single entry's estimated covariance can greatly alter its inverse, which in turn can effect the portfolio weights [@lopez]. This is exacerbated by the fact that covariance matrices themselves are prone to estimation error [@zhou2019]. For a sample with a given number of periods, larger dimension covariance matrices are prone to more noise in estimation. This is essentially due to a reduction in degrees of freedom as a sample of at least $1/2N(N+1)$ independent and identically distributed (iid) observations are required to estimate an $N\times N$ covariance matrix [@lopez, pp. 60]]. Furthermore, financial market covariance structures tend to vary over time and have been know to change rapidly during so-called regime changes [@lopez]. This exacerbates the issue of requiring a large number of observations when estimating the covariance matrix, as there is no guarantee that passed data will be a good refection of the future and looking further into the passed decreases the likelihood of it being so. 

". Markowitz’s curse is that the more correlated investments are, the
greater is the need for a diversified portfolio—and yet
the greater are that portfolio’s estimation errors. [@lopez]"

### Risk Based Portfolio's

This section reviews the intuition and technical underpinnings within the litrature surrounding the so-called risk-based portfolios. These include the equal weight (EW), minimum variance (MV), inverse volatility (IV), equal risk contribution and the maximum diversification portfolios. The EW is a simple heuristic approach, the minimum variance is more akin to a Markovitz (1952) mean variance portfolio, while the inverse-variance (IV), equal risk contrition (ERC) and maximum diversification (MD) are quite similar in that they each assume that adequate diversification can be obtained by allocating equal risk to each investible security.

#### Naive Equal Weight (EW)

Perhaps the oldest and most simple portfolio diversification heuristic constitutes holding a weight of $1/N$ of the $N$ total assets available to the investor [@demiguel2009]. Therefore, this stratagy doesn't require and data and doesn't involdve any kind of optimization [demiguel2009]. In layman's terms this can be describes as putting an equal number of eggs in each available basket. This portfolio is commonly called the equal weight or 1/N portfolio, its failure to recognize the importance of both the relative asset variance and the covariance between assets has also resulted in it being referred to as the naive portfolio. Its simplicity has resulted in it commonly being used as the benchmark portfolio. From a mean variance perspective this allocation is optimal when is no correlation between securities and each possesses the same variance [@Ref]. 

Despite its simplistic nature empirical studies tend to find a statistically insignificant difference in Sharp ratio between the naive portfolio and more advanced portfolio optimisers. This finding was made in @demiguel2009 who looked at the mean-variance, minimum-variance and Bayes-Stein portfolio's, where EW also performed surprisingly well from a total return perspective.

#### Minimum Variance (MV)

Portfolio optimisers designed to exhibit the minimum variance have more recently garnered a lot of attention, largely due their tendency to achieve surprisingly high returns in historical back tests [@clarke2011]. This performance has been attributed to the empirical phenomenon that low volatility stocks tend to earn returns in excess of the market, and high beta stocks tend not to be rewarded by higher returns [@clarke2011; @fama1992]. These findings are contrary financial economic theory. For example, the minimum variance (MV) portfolio tends to achieve cumulative returns equal to or slightly greater than market capitalization weighted portfolio's whilst maintaining consistently lower variance and achieving a noticeable improvement in downside risk mitigation, even during times of financial crisis [@clarke2011]. Interestingly, the MV portfolio is the only portfolio of the efficient frontier that does not depend on expected return forecasts[@lopez].

The minimum variance portfolio selects security weights such that the resulting portfolio corresponds to that with the lowest possible in sample volatility. Therefore, it has the lowest expected volatility and is, in theory, safest/least risky portfolio [@rawl2012]. Its primary input is a variance covariance matrix, which it uses to minimize aggregate portfolio volatility. This is accomplished by over-weighting low volatility and low correlation securities [@rawl2012]. 

Let $\sum$ indicate the markets variance covariance matrix and $w=\{w_i,..., w_N \}$ be a vector of length N containing individual security weights. The vector containing MV portfolio weights can new be described as [@rawl2012]:

\begin{center}
$w^*=arg\min(w'\sum w)\ \ \ s.t.\ \sum^N_iw_i=1$ 
\end{center}

This approach often works well out of sample, but if left unrestricted is known to build highly concentrated portfolio's [@lopez]. Its sole objective to minimize portfolio volatility has been cited as the primary reason for this. When near trough of its objective function it to achieves minor reductions in _ex ante_ volatility by greatly favoring a small number of low volatility/correlation securities [@lopez, pp. 68]]. This tendency to produce highly concentrated portfolio's be costly out of sample as the portfolio has not sufficiently diversifies idiosyncratic risk, it puts too many eggs in a small number of baskets. In practice This issue can  be countered by applying cleaver maximum and minimum constraints on portfolio weights.

#### Inverse-Varience (IV) Weighting

The IV portfolio, referred to as the equal-risk budget (ERB) portfolio in @leote, aims to allocate an equal risk budget to each investible security [@leote]. Where the risk budget is defined as the the product of a the security's weight and volatility. Therefore, if we define $\sigma_i$ as security i's volatility, then marginal volatility is equally distributed across N securities by setting security weights as such:

\begin{center} 
$w_{iv}=(\frac{1/\sigma_1}{\sum^N_{j=1} 1/\sigma}, ...,\frac{1/\sigma_N}{\sum^N_{j=1} 1/\sigma} )$ 
\end{center}

By this definition each portfolio's weight is proportional to its the inverse of its variance (hence its name). The IV portfolio's weighting strange implies that adequate diversification is attained when allocating capital according to individual security variances and thereby implying that it ignores the role that co-variations between securities on portfolio volatility. 

@leote found that, if all securities posses the same sharp ratio and correlation coefficients between each security are equal, then the IV portfolio is efficient from a mean variance stand point and obtains the highest possible sharp ratio. 

#### Equal Risk Contribution (ERC)

The ERC portfolio is similar to the IV, but also takes the covariance between securities into account when balancing risk contributions [@leote]. The basic idea behind the ERC is to weight the portfolio such that each security contributes equally to overall portfolio risk, which in turn maximises risk diversification [@maillard2010]. Generally speaking the ERC acts similar to a weight constrained MV portfolio, with constraints ensuring that an adequate level idiosyncratic risk is diversification. Following @maillard2010, the weights of an ERC portfolio $x=(x_1,x_2,...,x_n)$ consisting of n assets can be calculated as follows:

let $\sigma_i^2$ resemble asset i's variance, $\sigma_{ij}$ the covariance between asset i and j and $\sum$ be the markets variance covariance matrix. Portfolio risk can now be written as $sigma(x)=\sqrt{x^T\sum x}=\sum_i\sum_{j\neq i}x_ix_j\sigma_{ij}$ and the marginal risk contribution $\partial_{x_i}\sigma(x)$ can then be defined as as such:

\begin{center}
$\partial_{x_i}\sigma(x)=\frac{\partial\sigma(x)}{\partial x_i}=\frac{x_i\sigma_i^2+\sum_{j\neq i}x_j\sigma_{ij}}{\sigma(x)}$ 
\end{center}

Therefore, $\partial_{x_i}\sigma(x)$ refers to the change in portfolio volatility resulting from a small change in asset i's weight [@maillard2010]. ERC uses this definition to guide its algorithms central objective to equate the risk contribution for each asset in the portfolio _ex ante_. No closed form solution exists describing the weigts of the ERC portfolio, however, if we define $(\sum x)_i$ as the $i^{th}$ row resulting from the product of $\sum$ with x and note that $\partial_{x_i}\sigma(x)=(\sum x)_i$, then the optimal weight for the long only ERC can be described as those that satisfy the following statement:

\begin{center}
$x^*=\{x \ \epsilon[0,1]^n:\sum x_i=1, x_i \times (\sum x)_i=x_j \times (\sum x)_j \ \forall  \ i,j \}$ 
\end{center}

@maillard2010 proved mathematically that the ERC portfolio's _ex ante_ volatility is always some where between those of the EW and MV portfolio's. Thereafter, @leote found that, if all securities posses the same sharp ratio , then the ERC and ERB have identical portfolio weights. If in addition the correlation coefficients between all securities are equal, then the ERC and ERB merge into the EW portfolio and each are mean variance efficient with the maximum sharp ratio [@leote]. 

#### Maximum Diversification (MD)

@choueifaty2008 originally designed the MD portfolio to maximize some diversification ratio (DR), hwich he defigned as the sum of each securities risk bucket divided by portfolio volatility [@leote]. If we define $w=(w_1,...w_N)^T$ as a vector of portfolio weights, V as a vector of asset volatilities and $\sum$ as the covariance matrix. Then the DR can be expresses as:

\begin{center} 
$DR= \frac{w'.V}{\sqrt{w'Vw}}$ 
\end{center}

Therefore, much like the IV and ERC portfolio's, the MD portfolio attempts to diversify the portfolio by allocating equal risk to each security [@choueifaty2008]. The MD portfolio accomplishes this by over-weighting low volatility securities and those that are less correlated with other stocks [@leote]. For further detail regarding the theoretical results and properties of the MD portfolio see @choueifaty2008 [pp. 33-35].


## Empirical Backtests and Monte Carlo Findings

@choueifaty2013 conducted an empirical back test comparing the relative performance if numerous portfolio optimisers between 1999 and 2010. They used historical data from the MSCI World world index and considered the largest 50% of assets at each semi-annual rebalance date. To reduce the noise in estimation, at each rebalance date covariance matrices were estimated using the previous years worth of data [@choueifaty2013]. These were then used as the primary inputs in estimating the respective portfolio weights, which were restricted to long only. The MV portfolio achieved an annual return of 6.7% and outperformed the ERC and EW portfolio's who returned 6.3% and 5.8% respectively. Unsurprisingly, the MV portfolio possessed the lowest daily volatility (10%) followed by the ERC and then the EW portfolio's (with 12.9% and 16.4% respectively). Accordingly the MV portfolio scored the highest sharp ratio (0.36) followed by the ERC and EW portfolio's (0.24 and 0.16 respectively). According to @leote the performance of the EW portfolio primarily depends on the premium on small-capitalization stocks, thereby suggesting that the relatively poor performance of the EW portfolio in @choueifaty2013, can be attributed to the relatively poor returns achieved by the smaller stocks in the MSCI world index. 

Due to the aforementioned issues surrounding estimation error in a market's covariance matrix @ardia2017 set out to evaluate the impact of covariance matrix misspecification on the properties of risk based portfolio's. They used Monte Carlo methods to build six distinctive investment universes, each with a unique, variance/correlation structure and number of assets. Various covariance matrix estimation techniques were then estimated on the simulated data, with one serving as the benchmark. They then used the simulated data and the various covariance matrices to access the impact of alternative covariance specifications on the performance of the MV, IV, ERC and MD portfolio's. The ERC and IV portfolios were found to be "relatively robust to covariance misspecification", the MV to be sensitive to mispecification in both the variance and covarienve and the MD portfolio to be robust to misspecification in the variances but sensitive to misspecification in the covariances [@ardia2017, pp. 1].

## Monte Carlo Methods in Portfolio Optimisation

From the onset of electronic computing people have shown a keen interest in using said computing power to conduct randomized experiments [@kroese2014, pp. 1]. The core of Monte Carlo simulations is the creation of random objects or processes using a computer. There are a number of reasons for doing this, but the primary one used in this work and thereby discussed in this review is of the sampling kind [@kroese2014]. This typically involves the modeling of some stochastic object or process, followed by sampling from some probability distribution and manipulating said sample through some deterministic process such that the underlying process mimics the true underlying process. The primary idea behind Monte Carlo simuation is to repeat the simulation process many times so that interesting properties can be uncovered through the law of large numbers.\

An example of this can be found in @wang2012 who designed a Monte Calro procedure that (1) models both the time-series and cross-section properties of financial market returns, this also involves the estimation of a random term's probability distribution function (pdf). And (2) sampling from the modeled process to produce an ensemble of market, with each exerting the same risk properties. The simulated data can then be used for risk management and/or the pricing of securities [@wang2012; @kroese2014]. This unique ability to generate a large number of counterfactuals for an asset market with a known risk structure has made it a uniquely powerful tool in accessing the properties of portfolio optimization algorithms [@lopez2012]. 

@glasserman2013 is a  useful source for understanding the methods and applications of Monte Carlo methods in finance.

# Methadology

This work used Monte Carlo simulation methods to investigate the link between a markets correlation structure and the relative performance of the EW, MV, IV, ERC and MD portfolios. To avoid possible confusion note the following terminology: the term market refers to the daily returns for a number of assets, but since this is a Monte Carlo study, a market can be thought of as a single observation. The term market type refers to an set or ensemble of markets each with the same specified same risk characteristics. 

The R package MCmarket was used to simulate five distinctive market types, each corresponding to a unique correlation structure [@mcmarket]. Four of the correlation matrices were designed _ad hoc_, to posses a unique correlation structure, while the fifth was estimated using empirical data from the constitutes of the S&P 500. These correlation matrices range from one exhibiting no correlation (i.e. a diagonal matrix) to one with hierarchical clustering (see \ref{corr_struc}). 

The long only EW, MV, IV, ERC and MD portfolios were then backtested on the simulated markets and portfolio analytics were performed for each portfolio type in each market within the five market types. These portfolio analytics include the standard deviation (sd) of daily returns, downside deviation, value at risk (VaR), conditional VAR (CVaR), Sharp ratio, average drawdown and maximum drawdown. The mean and median of the portfolio metrics are then calculated for each market type.

Finally, the portfolio metrics are compared within portfolio types across market types and within market types across portfolio types. 

## Correlation Structures \label{corr_struc}

This section first describes the composition and attributes behind the four _ad hoc_ correlation matrices used in this study (section \ref{adhoc}). Secondly, the methodology behind the estimation of the empirical correlation matrix (section \ref{emp) is discussed and finally each of their top 10 eigenvalues are listed in Table \ref{eigens}. 

### Ad Hoc \label{adhoc}

This section sets out the four _ad hoc_ 50 by 50 correlation matrices used as the key inputs in their respective Monte Carlo simulations. Figure \ref{corr_mats} presents the said correlation matrices.

The first and most simplistic of the four matrices (labeled Diagonal Matrix in Figure \ref{corr_mats}) describes a market with a zero correlation coefficient between each asset. Each of its 50 eigenvalues are 1. It has no risk clusters and is therefore plenty scope for diversification. 

The second matrix (labeled No Clusters in Figure \ref{corr_mats}) has no risk clusters but describes a market with significant correlation between its constituents. Each asset has a correlation of 0.9 with is closest neighbor (i.e. Asset 1 and 2, 5 and 6 and 11 and 12 each have a pairwise correlation coefficient of 0.9). Correlations then diminish exponentially by the absolute distance between the two assets (i.e. the correlation between Asset 1 and 5 is $0.9^{|1-5|}=0.6561$). Its has a large first eigenvalue of 15.93, but they quickly diminish such that its 9th eigenvalue is less than 1 at 0.79. __COMMENTS__

The third matrix (labeled Five Clusters in Figure \ref{corr_mats}) contains five distinctive non-overlapping risk clusters. Assets within the same cluster have a pairwise correlation coefficient of 0.6 while those in separate clusters are not correlated. Its first five eigenvalues are around 6.5, with the remaining 45 at 0.4. __COMMENTS__

The final _ad hoc_ correlation matrix has three layers of overlapping risk clusters. The first layer has 10 distinctive clusters within which assets have a correlation coefficient of 0.7. The second layer has four clusters where assets that are not in same first layer cluster have a correlation coefficient of 0.5. Assets that are in the same third layer cluster but not clustered in layers one and two have a correlation coefficient of 0.3. Finally, those who do not share any cluster have a correlation coefficient of 0.05. Its largest eigenvalue is 14.36, but they quickly diminish as its third largest is only 3.3. 

See Table \ref{eigens} for a list of each correlation matrices largest ten eigenvalues. 

```{r corr mats, fig.cap="\\label{corr_mats} Correlation Matricies"}
pacman::p_load(MCmarket, tidyverse, patchwork, ggcorrplot)

corr_1 <- diag(50) %>% ggcorrplot(hc.order = TRUE, title = "Diagonal Matrix", show.legend = FALSE)
corr_2 <- gen_corr(D = 50, clusters = "none") %>% ggcorrplot(title = "No Clusters", show.legend = FALSE)
corr_3 <- gen_corr(D = 50, clusters = "non-overlapping", num_clusters = 5) %>%
  ggcorrplot(hc.order = TRUE, title = "Five Clusters", show.legend = FALSE)
corr_4 <- gen_corr(D = 50, clusters = "overlapping", 
                   num_clusters = c(10,5,2), num_layers = 3) %>% 
  ggcorrplot(hc.order = TRUE, title = "Overlapping Clusters", show.legend = FALSE)

my_title <- expression(paste(italic("ad hoc"), " Correlation Matrices"))

(corr_1 + corr_2) / (corr_3 + corr_4) + 
  patchwork::plot_annotation(title = my_title,
                             theme = theme(plot.title = element_text(hjust = 0.3, size=15))) 
  
```

### Emperical \label{emp}

The empirical correlation matrix used in this study was estimated from the daily returns of a random subset of 50 of the largest (by market capitalization) 100 S&P 500 stocks between 1 January 2016 and 1 January 2021. The market capitalizations were measured as of 12 January 2020. The covariance matrix was then calculated using the _fit_mvt_ function from the R package _fitHeavyTail_. This function uses maximum likelihood estimation and generalized expectation maximization method to fit a multivariate t-distribution to a matrix of asset returns [@liu1995]. 

The estimated multivariate t distribution was found to have 4.43 degrees of freedom and a correlation matrix shown in Figure \ref{corr_emp}. Note that the assets were ordered by hierarchical clustering so the reader can easily visualize the risk clusters. The correlation matrix's largest eigenvalue is 18.6 and they quickly diminish to below zero by its 8th largest eigenvalue.

```{r, fig.cap="\\label{corr_emp} Emperical Correlation Matrix"}
load(file = "data/emp_corr.rda")
ggcorrplot::ggcorrplot(emp_corr, title = "Emperical Correlation Matrix", hc.order = TRUE) +
  theme(legend.position = "none",
        axis.text.y = element_text(size=7),
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size=7))
```

### Correlation Matrix Eigen Values

```{r eigen tabel, results="asis"}
load("data/eigen_table.rda")
stargazer::stargazer(round(eigen_table, 2), header = FALSE, type = "latex", summary = FALSE, rownames = FALSE, 
                     label = "eigens", digits.extra = 2)
```


## Monte Carlo

This section outline the methodology behind the Monte Carlo simulation performed as part of this study. 

This work uses a generalized version of the Monte Carlo procedure developed in @wang2012. This framework was build into the R package _MCmarket_ which was used to carry out the simulations in this project [@potgieter]. The following briefly outlines the construction of the simulated markets:

An Elliptical t copula with 4.5 degrees of freedom is used, in conjunction with one of each of the five previously discussed correlation matrices, to specify the each market types multivariate distribution.  300 random uniformly distributed draws, corresponding to 300 trading days, were then taken from the specified multivariate distributions described by the t copula. The random uniformly distributed observations are then transformed, via the inverted normal cumulative distribution function, into normally distributed observations [@wang2012, pp. 3]. __The expected returns are set by randomly selecting 50 observations from the normal distribution with a mean of 0.02 and sd of 0.01. While the asset sd's are set by __
 

## Back Tests

This section describes the back testing procedure used to calculate the returns obtained by the respective EW, MV, IV, ERC and MD portfolios.

To remain consistent with the literature, a long-only weight constraint was applied to all portfolio's. An additional constraint limiting the maximum weight allocation for a single security to _20%_ is also applied to prevent some portfolios from building unreasonably highly concentrated holdings, while remaining flexible enough to punish those who do so. Therefore, these constraints act to provide a fair playing ground for the portfolio's to compete. The back testing procedure works as follows:

The first 100 periods are used to estimate the covariance matrix using the maximum likelihood methodology described in @liu1995. Interestingly, the assumption regarding the multivariate t distribution are correct by definition as the distribution used to for the monte Carlo simulations. This covariance matrix is then used as the sole input when calculating the respective portfolio weights. These weights are then used to calculate portfolio returns over the next 50 periods, when each portfolio is replaced by looking back another 100 periods to recalculated the covariance matrix. This process is repeated until the data is exhausted. Since 300 periods were simulated for each market each portfolio is weighted four times and 200 periods of returns are calculated for each portfolio within each market.

## Portfolio Analytics

This section describes the portfolio performance metrics used to evaluate and compare the respective portfolio's.

SD simple and widely accepted method of measuring portfolio volatility. 

The 95% Value at risk (VaR) is another risk metric used to evaluate the portfolio risk. It is one of the financial industry standards for measuring the downside risk [@PerformanceAnalytics]. It can be interpreted as the maximum return expected from in the worst 5% of scenarios, that is, in such a scenario, one should expect to loose at least their 95% value at risk. The Gaussian VaR is calculated by assuming that returns are normally $N(\mu,\sigma)$ distributed, where $\mu$ and $\sigma$ are estimated using historical data. In this study this assumption is correct by definition and will therefore result in an accurate estimate of downside risk. However, one must be careful when extrapolating these results as fairly contentious in real world returns. 

The 95% conditional value at risk (CVaR) or expected shortfall (ES) is another downside risk measure used to compare portfolio's. It is interpreted as the expected loss given that the portfolio is in the work 5% of scenarios.

The sharp ratio is calculated by dividing the portfolio return by some risk measure and is therefore interpreted as the return per unit of risk. In this work risk is measured as the sd of portfolio returns. 

The maximum drawdown is calculated by first, calculating portfolio cumulative returns and the maximum cumulative return achieved. The maximum drawdown is then the maximum amount that the cumulative return dips below its maximum, it is measured as a percentage of the maximum cumulative return [@PerformanceAnalytics]. 


# Results and Discussion
## Diag

With such a market structure it is reasonable to expect that, depending if the asset variance allocation is fairly evenly distribution then the EW, MV, IV, ERC and MD will produce fairly similar.

# Conclusion

I hope you find this template useful. Remember, stackoverflow is your friend - use it to find answers to questions. Feel free to write me a mail if you have any questions regarding the use of this package. To cite this package, simply type citation("Texevier") in Rstudio to get the citation for @Texevier (Note that united references in your bibtex file will not be included in References).


<!-- Make title of bibliography here: -->
<!-- \newpage -->

\newpage
# References {-}

<div id="refs"></div>

\newpage
# Appendix {-}

### Hierarchical Risk Parity (HRP)

Due to the multitude of robustness issues related to traditional portfolio optimisers, @lopez developed a new approach incorporating machine-learning methods and graph theory [@arevalo]. @lopez argues that the "lack of hierarchical structure in a correlation matrix allows weights to vary freely in unintended ways" and that this contributes to the instability issues. His HRP algorithm requires only a singular co-variance matrix and can utilize the information within without the need for the positive definite property [@lopez]. This procedure works in three stages:

@lopez carried out an in sample simulation study comparing the respective allocations of the long-only minimum variance, IVP and HRP portfolios using a co-variance matrix using a condition number that is "not unfavourable" to the minimum variance portfolio. The simulated data consisted of 10000 observations across 10 variables. The following findings were made: The minimum variance portfolio concentrated 92.66% of funds in the top 5 holdings and assigned a zero weight to 3 assets. _Conversly_, HRP only assigned 62.5% of its funds to the top 5 holdings [@lopez]. The minimum variance portfolio's objective function causes it to build highly concentrated portfolio's in favor of a small reduction in volatility; the HRP portfolio had only a slightly higher volatility [@lopez]. This apparent diversification advantage achieved by the minimum variance portfolio is rather deceptive as the portfolio remains highly susceptible to idiosyncratic risk incidents within its top holdings [@lopez]. This claim was further validated by the finding that HRP achieved significantly lower out of sample variance compared to the minimum variance portfolio.


## Appendix A {-}

Some appendix information here

## Appendix B {-}

