---
# IMPORTANT: Change settings here, but DO NOT change the spacing.
# Remove comments and add values where applicable.
# The descriptions below should be self-explanatory

title: "The Link Between Market Correlation Structure and the Performance of Risk-Based Portfolios"
#subtitle: "This will appear as Right Header"

documentclass: "elsarticle"

# --------- Thesis title (Optional - set to FALSE by default).
# You can move the details below around as you please.
Thesis_FP: FALSE
# Entry1: "An unbelievable study with a title spanning multiple lines."
# Entry2: "\\textbf{Nico Katzke}" # textbf for bold
# Entry3: "A thesis submitted toward the degree of Doctor of Philosophy"
# Uni_Logo: Tex/Logo.png # Place a  png logo in an img folder in your root and uncomment this. Leave uncommented for no image
# Logo_width: 0.3 # If using a logo - use this to set width (size) of image
# Entry4: "Under the supervision of: \\vfill Prof. Joe Smith and Dr. Frank Smith"
# Entry5: "Stellenbosch University"
# Entry6: April 2020
# Entry7:
# Entry8:

# --------- Front Page
# Comment: ----- Follow this pattern for up to 5 authors
AddTitle: TRUE # Use FALSE when submitting to peer reviewed platform. This will remove author names.
Author1: "Nathan Potgieter^[__Contributions:__  \\newline _The author would like to thank Nico Katzke for helping me puzzle and prod my to to the eventual completion of this research project._]"  # First Author - note the thanks message displayed as an italic footnote of first page.
Ref1: "Stellenbosch University, Stellenbosch, South Africa" # First Author's Affiliation
Email1: "19959672\\@sun.ac.za" # First Author's Email address

#Author2: "John Smith"
#Ref2: "Some other Institution, Cape Town, South Africa"
#Email2: "John\\@gmail.com"
#CommonAffiliation_12: TRUE # If Author 1 and 2 have a common affiliation. Works with _13, _23, etc.

#Author3: "John Doe"
#Email3: "Joe\\@gmail.com"

#CorrespAuthor_1: TRUE  # If corresponding author is author 3, e.g., use CorrespAuthor_3: TRUE

keywords: "Monte Carlo \\sep Risk-based Portfolios \\sep Portfolio Selection \\sep Copula" # Use \\sep to separate
JELCodes: "L250 \\sep L100"

# ----- Manage headers and footers:
#BottomLFooter: $Title$
#BottomCFooter:
#TopLHeader: \leftmark # Adds section name at topleft. Remove comment to add it.
BottomRFooter: "\\footnotesize Page \\thepage" # Add a '#' before this line to remove footer.
addtoprule: TRUE
addfootrule: TRUE               # Use if footers added. Add '#' to remove line.

# --------- page margins:
margin: 2.3 # Sides
bottom: 2 # bottom
top: 2.5 # Top
HardSet_layout: TRUE # Hard-set the spacing of words in your document. This will stop LaTeX squashing text to fit on pages, e.g.
# This is done by hard-setting the spacing dimensions. Set to FALSE if you want LaTeX to optimize this for your paper.

# --------- Line numbers
linenumbers: FALSE # Used when submitting to journal

# ---------- References settings:
# You can download cls format here: https://www.zotero.org/ - simply search for your institution. You can also edit and save cls formats here: https://editor.citationstyles.org/about/
# Hit download, store it in Tex/ folder, and change reference below - easy.
bibliography: Tex/ref.bib       # Do not edit: Keep this naming convention and location.
csl: Tex/harvard-stellenbosch-university.csl # referencing format used.
# By default, the bibliography only displays the cited references. If you want to change this, you can comment out one of the following:
#nocite: '@*' # Add all items in bibliography, whether cited or not
# nocite: |  # add specific references that aren't cited
#  @grinold2000
#  @Someoneelse2010

# ---------- General:
RemovePreprintSubmittedTo: TRUE  # Removes the 'preprint submitted to...' at bottom of titlepage
Journal: "Journal of Finance"   # Journal that the paper will be submitting to, if RemovePreprintSubmittedTo is set to TRUE.
toc: FALSE                       # Add a table of contents
numbersections: TRUE             # Should sections (and thus figures and tables) be numbered?
fontsize: 11pt                  # Set fontsize
linestretch: 1.2                # Set distance between lines.
link-citations: TRUE            # This creates dynamic links to the papers in reference list.

### Adding additional latex packages:
header-includes:
    - \usepackage{amsmath} # Add additional packages here.

output:
  pdf_document:
    keep_tex: TRUE
    template: Tex/TexDefault.txt
    fig_width: 3.5 # Adjust default figure sizes. This can also be done in the chunks of the text.
    fig_height: 3.5
abstract: |
  This work uses Monte Carlo methods to design and simulate financial market returns for five distinctive markets types, with each market type possesing a unique correlation structure. The equal weight, minimum variance, inverse variance, equal risk contribution and maximum diversification risk-based portfolios are evaluated in each of the simulated markets. The relative performance of each of the portfolios are compaired within market types and the relationship between each portfolio's return characteristics and the market covariance structure is evaluated. __FINDINGS__
---

<!-- First: Set your default preferences for chunk options: -->

<!-- If you want a chunk's code to be printed, set echo = TRUE. message = FALSE stops R printing ugly package loading details in your final paper too. I also suggest setting warning = FALSE and checking for warnings in R, else you might find ugly warnings in your paper. -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 6, fig.height = 5, fig.pos="H", fig.pos = 'H')
# Note: Include = FALSE implies the code is executed, but not printed in your pdf.
# warning and message = FALSE implies ugly messages and warnings are removed from your pdf.
# These should be picked up when you execute the command chunks (code sections below) in your rmd, not printed in your paper!

# Lets load in example data, and see how this can be stored and later called from your 'data' folder.
if(!require("tidyverse")) install.packages("tidyverse")
library(tidyverse)
Example_data <- Texevier::Ex_Dat

# Notice that as you are working in a .Rproj file (I am assuming you are) - the relative paths of your directories start at your specified root.
# This means that when working in a .Rproj file, you never need to use getwd() - it is assumed as your base root automatically.
write_rds(Example_data, path = "data/Example_data.rds")

```


<!-- ############################## -->
<!-- # Start Writing here: -->
<!-- ############################## -->

# Introduction \label{Introduction}

Since Harry Markovitz's (1952) seminal work on mean-variance portfolios scholars from around the globe have been aspiring to develop a robust algorithm capable of situating a portfolio on the efficient frontier _ex ante_. There are now a wide array of available portfolio algorithms raging from simple heuristic based approaches to advanced mathematical algorithms based on quadratic optimization, random matrix theory and machine learning methods; with many more are still to come.

Unfortunately, portfolio optimisers of @markowitz mean-variance type suffer from seveer sensitivity issues, where slight changes in their expected return input cause large changes in optimal portfolio weights. This is exacerbated by the fact that expected returns are notoriously difficult, if not impossible, to accurately forecast [@lopez]. Due to this issue, this work focuses solely on the so-called risk-based portfolio, defined by @leote as "systemic quantitative approaches to portfolio allocation" that solely rely on views of risk when allocating capital. These strategies do not require expected return forecasts and are therefore said to be more robust to estimation error. Despite their sole focus on risk mitigation empirical back tests have shown that they often perform surprising well, form a total return standpoint [@choueifaty2013]. 

Rather than using the standard empirical approach to evaluating said strategies this work opts to use Monte Carlo simulation methods. This also allows for an investigation of the link between the markets covariance structure and portfolio performance. Monte Carlo methods prove to be invaluable in answering this question as they allow for the creation of _ad hoc_ markets with predetermined risk return characteristics, and hence leave no uncertainty regarding the composition of the market. This creates an environment ideal for experimentation as the researcher has complete control over the market and can therefore adjust the independent variable, in this case the markets correlation structure, and observe the response in the dependent variable, which in this study is the portfolio return series. Monte Carlo methods are also beneficial here because they enable the effective reduction of noise in portfolio performance measures.   

The risk-based portfolios evaluated in this work include the naive equal weight (EW), minimum variance (MV), inverse variance (IV), equal risk contribution (ERC) and the maximum diversification (MD) portfolios. Section \ref{aims} lays out this works aims and objects. Section \ref{lit} provides a review of the relevant literature; this includes some general issues plaguing the field of portfolio optimization, the rational and theoretical underpinnings behind the five risk-based portfolios, their relative performance in empirical back tests and finally the importance of using Monte Carlo methods in finance. Section \ref{methadology} discusses this works methodology, Section \ref{reasults} provides and discusses the results, and finally Section \ref{conclusion} concludes. 

# Aims and Objectives \label{aims}

This work aims to use Monte Carlo Methods to uncover the relationship between a market's correlation structure and the risk return properties of various risk-based portfolio algorithms. This is achieved through the following objectives:

1. Design and create four distinctive _ad hoc_ correlation matrices and estimate one empirical correlation matrix, each describing a market of 50 assets with a unique correlation structure. These market range from those possessing no clusters to those exhibiting hierarchical clustering. All other risk characteristics remain equal between market types.

2. Use these five correlation matrices as the key input in their own Monte Carlo simulation. Where each correlation matrix corresponds to a unique market type. The markets will each be built with a student t multivariate distribution with 4.5 degrees of freedom. The individual asset's univariate distributions will each be normally distributed with means and standard deviations calibrated with S&P500 data. Each of the five market types will be simulated 10 000 times across 500 periods, or approximately two years of trading daily return data.  

3. Calculate the returns obtained from various risk based portfolio's in each of the simulated markets. Use the first __250__ periods to estimate an out of sample covariance matrix and calculate portfolio weights. Conduct periodic rebalancing every 50 periods, each time looking back _250_ periods, recalculating the covariance matrix and the new portfolio weights. Repeat this process until all _500_ simulated periods have been considered.

4. Calculate the average Sharp ratio, standard deviation, downside deviation, value at risk (VaR), effective number of constitutes (ENC) and effective number of bets (ENB) for each portfolio across the 10 000 markets for each market type. 

5. Compare the relative performance of each portfolio within each of the market types and evaluate how the relative performance of each portfolio is effected by the change in market correlation structure. 

# Litrature Review \label{lit}

## A Review of Portfolio Optimisation Algorithms

### Introduction

This literature review will cover some common issues found in the literature surrounding portfolio optimization, the five risk-based portfolios evaluated in this work, their respective performance in both empirical back tests and Monte Carlo studies and finally the importance of using Monte Carlo methods within the field of finance. 

### Common Issues Portfolio Optimizers
 
When operating in sample, Portfolio optimization tends to be a perfect science, but out of sample it becomes more of an art form where it is often preferable to use heuristic over hard rules. This section highlights some general issues, highlighted within the portfolio optimization literature, that tend to worsen their performance out of sample.

Firstly, mean-variance optimisers, like those introduced by @markowitz, rely heavily on the accuracy of their expected return forecasts. Small changes in their expected return input can lead to large changes in portfolio weights [@lopez]. Since in practice expected returns are extremely difficult to accurately estimate, this issue serves as a major hindrance to their wide spread use. Due to this the so-called risk based portfolios that intentionally avoid using expected return forecasts have garnered a lot of attention [@maillard2010]. 

However, these risk based portfolios are not void of issues. The quadratic programming methods used in many portfolio optimisers, including Markovitz's (1952) mean variance and some risk-based portfolios, require the inversion of some positive-definite covariance matrix. This requirement for positive definiteness can cause issues as covariance matrices estimated on empirical data are sometimes not positive definite, in which case their inverse does not exist and these portfolios don't have solutions [@lopez]. A common method to get around this issue is to simply compute the nearest positive definite matrix and use that instead [@higham2002; @Matrix]. 

The covariance estimation step is particularly susceptible to measurement error if the underlying covariance matrix suffers from a high condition number [@zhou2019]. A condition number is defined as the absolute value of the ratio between a covariance matrix's largest and smallest eigenvalues [@lopez; @lopez2012]. The condition number is smallest in diagonal matrices (they have a condition number of 1) and increases as more correlated variables are added. When working with high condition number matrices a small change in a single entry's estimated covariance can greatly alter its inverse, which in turn can effect the portfolio weights [@lopez]. This is related to Markowitz's curse which @lopez summerised by stating that "the more correlated investments are, the greater is the need for a diversified portfolio—and yet the greater are that portfolio’s estimation errors". 

For a sample with a given number of periods, larger dimension covariance matrices are prone to more noise in estimation. This is essentially due to a reduction in degrees of freedom as a sample of at least $1/2N(N+1)$ independent and identically distributed (iid) observations are required to estimate an $N\times N$ covariance matrix [@lopez, pp. 60]]. Furthermore, financial market covariance structures tend to vary over time and have been know to change rapidly during so-called regime changes [@lopez]. This exacerbates the issue of requiring a large number of observations when estimating the covariance matrix, since passed data may not be a good refection of the future and looking further into the passed increases this likelihood. 

### Risk Based Portfolio's

This section reviews the intuition and technical underpinnings within the literature surrounding risk-based portfolios. Those discussed here include the equal weight (EW), minimum variance (MV), inverse volatility (IV), equal risk contribution (ERC) and the maximum diversification (MD) portfolios. The EW is a simple heuristic approach, the minimum variance is more akin to a Markovitz (1952) mean variance portfolio, while the inverse-variance (IV), equal risk contrition (ERC) and maximum diversification (MD) are quite similar in that they all assume that adequate diversification can be obtained by allocating equal risk to each investible security.

#### Naive Equal Weight (EW)

Perhaps the oldest and most simple portfolio diversification heuristic involves holding a weight of $1/N$ of the $N$ total available assets [@demiguel2009]. In other words this strategy can be described as putting an equal number of eggs into each available basket. It doesn't require any historical data when allocating capital and doesn't involve any form of optimization [@demiguel2009]. This portfolio is commonly called the equal weight or 1/N portfolio, however its failure to recognize the importance of both asset variance and the covariance between assets has resulted in it also being referred to as the naive portfolio. Meanwhile its simplicity means that it has been widely used as a benchmark. Equal weighting is optimal from a mean variance standpoint when there is no correlation between securities and each possesses the same variance. In which case, the EW is theoretically equivalent to the MV portfolio. The EW's robustness to estimation errors and surprisingly good historical performance has lead to it being incorporated into hybrid portfolio strategies [@tu2011]. These strategies form a balance between the rules based approach of the EW and the optimization approaches from more sophisticated portfolios and have been shown to outperform both its EW and more sophisticated constituents. 

#### Minimum Variance (MV)

Portfolio optimisers designed to exhibit the minimum variance have in recent years garnered a lot of attention. This can in large part be attributed to their tendency to achieve surprisingly high returns and low variance in historical back tests [@clarke2011]. Their excelent performance has been attributed to the empirical phenomena that low volatility stocks tend to earn returns in excess of the market, and high beta stocks tend not to be rewarded by higher returns [@clarke2011; @fama1992]. Interesting this later finding is contrary to traditional financial economic theory which predicts an asset's expected return to be proportional to its market beta (i.e. undiversifiable risk) [@perold2004]. 

The minimum variance portfolio selects security weights such that the resulting portfolio corresponds to that with the lowest possible in sample volatility. Therefore, it has the lowest expected volatility and is, in theory, the safest/least risky portfolio [@rawl2012]. Its primary input is a variance covariance matrix, which it uses to minimize aggregate portfolio volatility. This is accomplished by over-weighting low volatility and low correlation securities [@rawl2012]. Interestingly, it is the only portfolio of the efficient frontier that does not depend on expected return forecasts [@lopez].

Let $\sum$ indicate the markets variance covariance matrix and $w=\{w_i,..., w_N \}$ be a vector of length N containing individual security weights. The vector containing the MV portfolio's weights can now be described as [@rawl2012]:

\begin{center}
$w^*=arg\min(w'\sum w)\ \ \ s.t.\ \sum^N_iw_i=1$ 
\end{center}

In some studies the minimum variance (MV) portfolio has been found by to earn cumulative returns equal to or slightly greater than market capitalization weighted portfolio's whilst maintaining a consistently lower variance and achieving a noticeable improvement in downside risk mitigation [@clarke2011]. The MV portfolio can therefore work well out of sample, but if left unrestricted is known to build highly concentrated portfolio's [@lopez]. Its sole objective to minimize portfolio volatility is likely the the primary reason for this. When near the trough of its objective function it to achieves minor reductions in _ex ante_ volatility by greatly favoring a small number of low volatility/correlation securities [@lopez, pp. 68]]. This tendency to produce highly concentrated portfolio's can be costly out of sample as the portfolio does not sufficiently diversify its idiosyncratic risk. It puts too many eggs in too few baskets. In practice This issue can  be countered by applying cleaver maximum and minimum portfolio weight constraints.

#### Inverse-Varience (IV) Weighting

The IV portfolio, referred to as the equal-risk budget (ERB) portfolio in @leote, aims to allocate an equal risk budget to each investible security [@leote]. Where the risk budget is defined as the the product of a security's weight and volatility. If $\sigma_i$ is defigned as security i's volatility, then risk the portfolio risk budget can be equally distributed across N securities by setting security weights as:

\begin{center} 
$w_{iv}=(\frac{1/\sigma_1}{\sum^N_{j=1} 1/\sigma}, ...,\frac{1/\sigma_N}{\sum^N_{j=1} 1/\sigma} )$ 
\end{center}
 
This indicates that each securities weight is directly proportional to the inverse of its variance, thereby demonstrating why this is called the IV portfolio. The IV portfolio allocates capital based solely on security variance and is therefore oblivious to the covariance between its constitutes. @leote found that, if all securities posses the same sharp ratio and their correlation coefficients are all equal, then the IV portfolio is efficient from a mean variance stand point and obtains the highest possible sharp ratio. 

#### Equal Risk Contribution (ERC)

The principle behind the ERC portfolio is similar to that of the IV, however when balancing risk contributions the ERC does account for the covariance between securities [@leote]. The ERC allocates capital such that each security contributes equally to overall portfolio risk, which in theory should maximize risk diversification [@maillard2010]. In practice the ERC acts similar to a weight constrained MV portfolio, with constraints preventing high levels of portfolio concentration. Following @maillard2010, the weights of an ERC portfolio $x=(x_1,x_2,...,x_n)$ consisting of n assets can be calculated as follows:

let $\sigma_i^2$ resemble asset i's variance, $\sigma_{ij}$ the covariance between asset i and j and $\sum$ be the markets variance covariance matrix. Portfolio risk can now be written as $sigma(x)=\sqrt{x^T\sum x}=\sum_i\sum_{j\neq i}x_ix_j\sigma_{ij}$ and the marginal risk contribution, $\partial_{x_i}\sigma(x)$, can then be defined as:

\begin{center}
$\partial_{x_i}\sigma(x)=\frac{\partial\sigma(x)}{\partial x_i}=\frac{x_i\sigma_i^2+\sum_{j\neq i}x_j\sigma_{ij}}{\sigma(x)}$ 
\end{center}

Therefore, $\partial_{x_i}\sigma(x)$ refers to the change in portfolio volatility resulting from a small change in asset i's weight [@maillard2010]. ERC uses this definition to guide central objective to equate the risk contribution across each of the n asset. There is no closed form solution describing the weights of the ERC portfolio, however, if we define $(\sum x)_i$ as the $i^{th}$ row resulting from the product of $\sum$ with x and note that $\partial_{x_i}\sigma(x)=(\sum x)_i$, then the optimal weight for the long only ERC can be described as those that satisfy the following statement (see @maillard2010, p. 4-7 for more detail):

\begin{center}
$x^*=\{x \ \epsilon[0,1]^n:\sum x_i=1, x_i \times (\sum x)_i=x_j \times (\sum x)_j \ \forall  \ i,j \}$ 
\end{center}

@maillard2010 proved mathematically that the ERC portfolio's _ex ante_ volatility is always some where between those of the EW and MV portfolio's. @leote found that, if all securities posses the same sharp ratio , then the ERC and ERB have identical portfolio weights. If in addition the correlation coefficients between all securities are equal, then the ERC and ERB merge into the EW portfolio, with each being mean variance efficient with the maximum attainable sharp ratio [@leote]. 

#### Maximum Diversification (MD)

@choueifaty2008 originally designed the MD portfolio to maximize some diversification ratio (D), which he defined as the sum of each securities risk bucket ($w'.V$) divided by portfolio volatility [@leote]. If we define V as a vector of asset volatilities and $\sum$ as the covariance matrix and $w^*$ as the vector of MD portfolio weight. Then the $w*$ can be expresses as:

\begin{center} 
$w^* =arg\ max(DR)\ \ \ \ with \ \ \ \  DR= \frac{w'.V}{\sqrt{w'Vw}}$ 
\end{center}

@leote found that in practice the MV achieves a diversification ratio similar to that of the MD and that the difference between the two arise from the difference between the two arise from the MV's larger exposure to low residual volatility securities. Much like the IV and ERC portfolios, the MD portfolio attempts to diversify its portfolio by allocating equal risk to each security [@choueifaty2008]. The MD portfolio accomplishes this by over-weighting low volatility and low correlation securities [@leote]. For further detail regarding the theoretical results and properties of the MD portfolio see @choueifaty2008 [pp. 33-35].

## Empirical Backtests and Monte Carlo Findings

@choueifaty2013 used empirical back testing to compare the relative performance of numerous portfolio optimisers. They used historical data from the MSCI world index and considered the largest 50% of assets at each semi-annual rebalance date between 1999 and 2010. To reduce the noise in estimation, at each rebalance date covariance matrices were estimated using the previous years worth of data [@choueifaty2013]. These were then used as the primary inputs in estimating the long-only portfolio weights. The MV portfolio achieved an annual return of 6.7% and outperformed the ERC and EW portfolios who returned 6.3% and 5.8% respectively. The MV portfolio possessed the lowest daily volatility (10%) followed by the ERC and then the EW portfolio's (with 12.9% and 16.4% respectively). Accordingly the MV portfolio had the highest sharp ratio (0.36) followed by the ERC and EW portfolios (0.24 and 0.16 respectively).

Despite the EW portfolio simplistic nature empirical studies, like those by @demiguel2009, who compared the EW to the mean-variance, MV and Bayes-Stein portfolios, tend to find a statistically insignificant difference in Sharp ratio between the naive portfolio and those of the more advanced portfolio optimisers.In addition the EW also performed surprisingly well from a total return perspective. In fact many studies have found that the EW portfolio outperforms the mean variance and other more sophisticated portfolios based in financial theory [@tu2011].

Due to the aforementioned issues surrounding covariance matrix estimation error,  @ardia2017 set out to evaluate the impact of covariance matrix misspecification on the properties of risk based portfolio's. They used Monte Carlo methods to build six distinctive investment universes, each with a unique, variance/correlation structure and a varying number of assets. Numerous covariance matrix estimation techniques were then estimated on the simulated data, one of which serving as the benchmark. They then used the simulated data and the various covariance matrices to access the impact of alternative covariance specifications on the performance of the MV, IV, ERC and MD portfolio's. The ERC and IV portfolios were found to be "relatively robust to covariance misspecification", the MV was found to be sensitive to misspecification in both the variance and covariance and the MD portfolio was found to be robust to variances misspecification but sensitive to misspecification in the covariances [@ardia2017, pp. 1].

## Monte Carlo Methods in Portfolio Optimisation

Ever since the pioneering age of computers people have shown a keen interest in leveraging their ability to perform rapid calculations to conduct randomized experiments [@kroese2014, pp. 1]. The core of Monte Carlo simulation is in the creation of random objects and/or processes using a computer. There are a number of reasons for doing this, but the primary one used in this work and thereby discussed in this review is of the sampling kind [@kroese2014]. This typically involves the modeling of some stochastic object or process, followed by sampling from some probability distribution and the manipulating said sample through some deterministic process such that the result mimics the true underlying process. The primary idea behind Monte Carlo simulation is to repeat this simulation process many times so that interesting properties can be uncovered through the law of large numbers and central limit theorem.

A financial application of this can be found in @wang2012 who designed a Monte Calro procedure that (1) models both the time-series and cross-section properties of financial market returns, this involves the estimation of a random term's probability distribution function (pdf) using extreme value theory. And (2) sampling from the modeled process to produce an ensemble of market returns, with each exerting the same risk properties. The simulated data can then be used in risk management and/or the pricing of financial securities [@wang2012; @kroese2014]. This unique ability to generate a large number of counterfactuals for an asset market with a known risk structure has made it a uniquely powerful tool in accessing the properties of portfolio optimization algorithms [@lopez2012]. 

@glasserman2013 is a  useful source for understanding the methods and applications of Monte Carlo methods in finance.

# Methadology \label{methadology}

This work used Monte Carlo simulation methods to investigate the link between a markets correlation structure and the relative performance of the EW, MV, IV, ERC and MD portfolios. To avoid possible confusion note the following terminology: 

- The term market refers to a set daily returns for a number of assets. e.g. the daily returns for each of the JSE ALSI constitutes between 1 January 2019 and 1 January 2020. Since this is a Monte Carlo study, thousands of markets are simulated, they can therefore be thought of as a single observation from a population of markets.
- The term market type refers to a population of markets each with the same specified risk characteristics.

The R package MCmarket was used to simulate 10 000 markets from five separate market types, with the correlation structure being the only differentiating factor between between market types. [@MCmarket]. Four of the correlation structures/matrices were designed _ad hoc_, while the fifth was estimated using S&P 500 data. These correlation matrices range from one exhibiting no correlation (i.e. a diagonal matrix) to one with a hierarchical clustering structure (see \ref{corr_struc}). 

The long only EW, MV, IV, ERC and MD portfolios were then back tested on the simulated markets (Section \ref{backtest}) and portfolio analytics were calculated and aggregated across the 10 000 markets (Section \ref{portmet}). Finally, the portfolio metrics are compared within market types across portfolios and within portfolios, across market types.  

## Correlation Structures \label{corr_struc}

This section describes the composition and attributes of the four _ad hoc_ correlation matrices used in this study (section \ref{adhoc}) as well as the methodology behind the estimation of the empirical correlation matrix (section \ref{emp). Each of the five matrices top 10 eigenvalues are listed in Table \ref{eigens}.

### Ad Hoc \label{adhoc}

This section describes the four _ad hoc_ 50 by 50 correlation matrices used as the key inputs in their respective Monte Carlo simulations. See Figure \ref{corr_mats} for a graphical representation of each correlation matrix. Note that the _gen_corr_ function from the R package _MCmarket_ was used in the construction of the four _ad hoc_ matrices [@MCmarket]. 

The first and most simplistic of the four matrices is a diagonal matrix (see Diagonal Matrix in Figure \ref{corr_mats}). It describes a market with a zero correlation coefficient between each asset. Each of its 50 eigenvalues are equal to 1 (Table \ref{eigens}), it has no risk clusters and has plenty scope for diversification. This correlation matrix has the lowest possible condition number of 1. The Monte Carlo data set constructed using this matrix will be referred to as Market 1. 

The second matrix (labeled No Clusters in Figure \ref{corr_mats}) has no risk clusters but describes a market with significant correlation between its constituents. Each asset has a correlation of 0.9 with is closest neighbor (i.e. Asset 1 and 2, 5 and 6 and 11 and 12 each have a pairwise correlation coefficient of 0.9). Correlations then diminish exponentially by the absolute distance between the two assets (i.e. the correlation between Asset 1 and 5 is $0.9^{|1-5|}=0.6561$). Its has a large first eigenvalue of 15.93, but they quickly diminish such that its 9th largest eigenvalue is less than 1 at 0.79 (Table \ref{eigens}). This correlation matrix has the highest condition number of 302.4. The Monte Carlo data set constructed using this matrix will be referred to as Market 2. 

The third matrix (labeled Five Clusters in Figure \ref{corr_mats}) contains five distinctive non-overlapping risk clusters. Assets within the same cluster have a pairwise correlation coefficient of 0.6 while those that are not in the same cluster are uncorrelated. ItThis correlation matrix has a condition number of 16.1. The Monte Carlo data set constructed using this matrix will be referred to as Market 3. 

The final _ad hoc_ correlation matrix has three layers of overlapping risk clusters. The first layer has 10 distinctive clusters, within which assets have a correlation coefficient of 0.7. The second layer has four clusters where assets that are not in same first layer cluster have a correlation coefficient of 0.5. Assets that are in the same third layer cluster but not clustered in layers one and two have a correlation coefficient of 0.3. Finally, those who do not share any cluster have a correlation coefficient of 0.05. Its largest eigenvalue is 14.36, but they diminish fairly quickly as its third largest is only 3.3 (Table \ref{eigens}). This correlation matrix has a condition number of 47.9. The Monte Carlo data set constructed using this matrix will be referred to as Market 4. 

Table \ref{eigens} provides a list of the ten largest eigenvalues for each of the five correlation matrices. 

```{r corr mats, fig.cap="\\label{corr_mats} Correlation Matricies"}
pacman::p_load(MCmarket, tidyverse, patchwork, ggcorrplot)

corr_1 <-
  diag(50) %>% ggcorrplot(hc.order = TRUE,
                          title = "Diagonal Matrix",
                          show.legend = FALSE)
corr_2 <-
  gen_corr(D = 50, clusters = "none") %>% 
  ggcorrplot(title = "No Clusters", 
             show.legend = FALSE)

corr_3 <-
  gen_corr(D = 50,
           clusters = "non-overlapping",
           num_clusters = 5) %>%
  ggcorrplot(hc.order = TRUE,
             title = "Five Clusters",
             show.legend = FALSE)


corr_4 <- gen_corr(D = 50,
                   clusters = "overlapping",
                   num_clusters = c(10, 5, 2)) %>%
  ggcorrplot(hc.order = TRUE,
             title = "Overlapping Clusters",
             show.legend = FALSE)

my_title <-
  expression(paste(italic("ad hoc"), " Correlation Matrices"))

(corr_1 + corr_2) / (corr_3 + corr_4) +
  patchwork::plot_annotation(title = my_title,
                             theme = theme(plot.title = element_text(hjust = 0.3, 
                                                                     size = 15))) 
  
```

### Emperical \label{emp}

The empirical correlation matrix used in this study was estimated from the daily returns of a random subset of 50 of the largest (by market capitalization) 100 S&P 500 stocks between 1 January 2016 and 1 January 2021. The market capitalizations were measured as of 12 January 2020. 

The markets covariance matrix was then estimated using the _fit_mvt_ function from the R package _fitHeavyTail_ [@fitHeavyTail]. This covariance estimation method uses maximum likelihood estimation and generalized expectation maximization to fit a multivariate t-distribution the a matrix of asset returns [@liu1995]. This procedure found that the multivariate t distribution with 4.43 degrees of freedom and the correlation matrix shown in Figure \ref{corr_emp} best approximated the return series. 

Note that in Figure \ref{corr_emp} assets were ordered by hierarchical clustering so the reader could more easily visualize the risk clusters. The correlation matrix's largest eigenvalue is 18.6 and they quickly diminish to below zero by its 8th largest eigenvalue (Table \ref{eigens}). This correlation matrix has a condition number of 211.1. The Monte Carlo data set constructed using this matrix will be referred to as Market 5. 

```{r, fig.cap="\\label{corr_emp} Emperical Correlation Matrix", fig.width=4.5, fig.height=4.5}
load(file = "data/emp_corr.rda")
ggcorrplot::ggcorrplot(emp_corr, title = "Emperical Correlation Matrix", hc.order = TRUE) +
  theme(
    legend.position = "none",
    axis.text.y = element_text(size = 7),
    axis.text.x = element_text(
      angle = 90,
      vjust = 0.5,
      hjust = 1,
      size = 7
    )
  )
```

```{r eigen tabel, results="asis"}
load("data/eigen_table.rda")
stargazer::stargazer(
  round(eigen_table, 2),
  header = FALSE,
  title = "Eigenvalues",
  type = "latex",
  summary = FALSE,
  rownames = FALSE,
  label = "eigens",
  digits.extra = 2
)
```

## Monte Carlo \label{mc}

This section outlines the process behind the Monte Carlo simulations performed as part of this study. 

A generalized version of the Monte Carlo procedure developed in @wang2012 was used to simulate five distinctive market types. This framework was build into the R package _MCmarket_ which was used to conduct this project's Monte Carlo simulations [@MCmarket]. The following briefly describes the process:

An Elliptical t copula with 4.5 degrees of freedom is used, in conjunction with a 50 by 50 correlation matrix (Section \ref{corrs}), to simulate 500 random uniformly distributed draws (corresponding to 500 trading days or approximately two years worth of trading days) across the 50 assets. The uniformly distributed observations were then transformed into normally distributed observations, via the inverted normal cumulative distribution function [@wang2012, pp. 3; @MCmarket]. This process was repeated 10 000 times for each of the five correlation matrices/ market types set out in section \ref{corr_struc}. The correlation matrix is the only distinguishing factor between market types as all other factors remain equal. All in all, this process created 5 data sets each containing 10 000 markets with 50 assets and 500 periods. 

The expected returns and standard deviation of these simulated variables were calibrated using a random subset of 50 of the largest 100 S&P500 stocks (discussed in Section \ref{emp_corr}) between 1 January 2020 and 1 January 2021. Maximum likelihood estimation was used to fit the multivariate t distribution to the return series using the method developed by @liu1995. This produced a series of estimated means and variances which were used to calibrate the expected returns and standard deviations of the simulated variables (see Table \ref(msd)).

## Back Tests \label{backtest}

This section describes the back testing procedure used to calculate the returns obtained by the EW, MV, IV, ERC and MD portfolios. The process described relates to a single market and was therefore applied to each of the 50 000 markets simulated in this study. 

To remain consistent with the literature as well as the mandate for the majority of portfolio managers, a long-only weight constraint was applied to all portfolio's. In addition a constraint limiting the maximum weight applied of a single security to 10% is also applied. This prevents some portfolios from building unreasonably highly concentrated holdings, while remaining flexible enough to punish those who do so. These constraints therefore act to provide a fair playing ground for the portfolio's to compete. The back testing procedure works as follows:

The first 250 periods, approximately equivalent to one years worth of daily return data, are used to fit a multivariate t distribution via maximum likelihood and estimate a covariance matrix [@liu1995]. Interestingly, the identifying assumption used in this covariance matrix estimation method, that is that the data comes from a multivariate t distribution, is correct by definition since this is the distribution used to simulate the data set. The estimated covariance matrix is then used as the sole input when calculating the weights for each of the respective risk-based portfolios. Each portfolio holds these weights over the next 50 periods, when they are rebalanced by looking back 250 periods, calculating the covariance matrix and the new portfolio returns. This process is repeated until all periods in the data set are exhausted. Since there are 500 periods in each market, each portfolio is weighted 5 times and 250 periods of daily returns are calculated for each portfolio.

## Portfolio Analytics \label{portmet}

This section describes the portfolio performance and concentration metrics used to evaluate and compare portfolios. The Sharp ratio is used to evaluate the risk adjusted return, while the standard deviation (SD), downside deviation (DD) and value at risk (VaR) are used to access portfolio risk. Finally, the effective number of constitutes, calculated as the inverse of the Herfindahl-Hirschman index (HHI), and the effective number of bets, calculated following @meucci2010, are used to compare diversification between portfolios [@rhoades1993]. 

Since @markowitz variance of returns has been the standard measure for risk in the financial industry [@meucci2010]. With SD simply being the square-root of the variance, it too is widely used and benefits due to its relative ease in interpretation. SD is also key in calculating the next two portfolio performance metrics described in this study, namely the Sharp ratio and value at risk (VaR).

The Sharp ratio is a measure of a portfolio's risk adjusted returns. Generally speaking, the Sharp ratio is calculated by dividing the portfolio return by some measure of portfolio risk, it is therefore interpreted as the return per unit of risk. In this work SD is used as the measure of risk. 

The 95% VaR is another risk metric used to evaluate portfolio risk performance in this study. It is one of the financial industry standards for measures for downside risk and can be interpreted as the maximum return expected from in the worst 5% of scenarios [@PerformanceAnalytics]. That is, in the worst 5% of scenarios, one should expect to loose at least this amount. The particular version of VaR used here is the Gaussian VaR, which is calculated by assuming that returns are normally $N(\mu,\sigma)$ distributed, where $\mu$ and $\sigma$ are estimated using historical data. The probability distribution assumptions allows one to attach a probability values to possible future portfolio returns. This assumption can be dangerous in practice, however in this study it correct by definition as the return series were each simulated to be normally distributed. It should therefore result in an accurate estimate of downside risk.

The HHI estimates portfolio concentration by  by summing the the portfolio weights squared [@rhoades1993]. A portfolio with small weights allocated evenly across a large number of securities will have an HHI of approximately zero, while a portfolio with all its capital invested in a single security will have the maximum HHI of 10000. The effective number of constitutes (ENC) can then be calculated as the inverse of the HHI, where an equally weighted portfolio with have an ENC equal to the number of securities and more concentrated portfolio's will have a ENC less than the number of securities. Weight based measures of portfolio diversification are severely limited in that they are oblivious to covariation between portfolio components. The ENC can therefore be misleading in financial applications where portfolio components are known to exhibit significant dependence. 

@meucci2010 attempted to rectify this issue when he introduced a new method to evaluate portfolio diversification that considers the portfolio's risk structure. He used a principle component (PC) approach to estimate the total number of orthogonal bets within a portfolio, which he simply referred to as principle portfolios. With this he estimated a portfolio diversification distribution using the percentage of total portfolio variation attributed to each principle portfolio. The effective number of orthogonal bets (ENB) can then be calculated as the dispersion of the diversification distribution [@meucci2010, p. 10].

# Results and Discussion \label{reasults}

Note that the markets simulated using the diagonal correlation matrix described in Section \ref{adhoc} will hence forth be referred to as Market 1. Similarly, the markets simulated using the no cluster, five clusters, overlapping clusters and empirical correlation matrices will be respectively referred to as Market 2, Market 3, Market 4 and Market 5. Therefore, each of the Markets 1 - 5 contain a unique correlation structure.

## Comparing Portfolios Within Market Types

### Market 1

The portfolios compared here were estimated on the markets simulated using the diagonal correlation matrix (Figure \ref{corr_mats}). Since there is no correlation between assets in this market type, its simulated markets are argubaly the least realistic. According to the portfolio risk metrics reported in Table \ref{rm1} the EW portfolio performed the best overall. It achieved the highest average Sharp ratio and the lowest average SD, DD and VaR. The IV portfolio was a close second, it has the second highest Sharp ratio, tied the lowest SD, narrowly placing second lowest in the DD and VaR metrics. The ERC ranked the third best overall, followed by the MD and finally the MV.

Since this market type has real no correlation structure it is unsurprising that the two portfolios that neglect using covariance information have performed relatively well. However, since the variance does differ between assets it would have been reasonable to assume that the IV portfolio would be less volatile than the EW. The fact that it is not may be due to (1) asset return variances are not being sufficiently different and/or (2) the IV portfolio's out of sample variance forecast not being accurate enough to reduce the IV portfolio's out of sample risk to below that of the EW portfolio.

The MV, ERC and MD portfolios performed  poorly compared to the EW and IV portfolios. This is likely be due to there being no real dependence in the underlying market's correlation structure. Therefore, these portfolios are more noisy in comparison as they use spurious covariance information when allocating capital. The EW and IV portfolio in comparison effectively assume that there is no correlation between assets. In Market 1 this assumption is correct by construction. 

```{r market 1, results = 'asis'}
source("code/table_risk_metrics.R")
load("data/perf_m1.rda")

risk_metrics_m1 <- table_risk_metrics(perf_m1)
stargazer::stargazer(
  risk_metrics_m1,
  header = FALSE,
  title = "Market 1 - Portfolio Risk Metrics",
  type = "latex",
  summary = FALSE,
  rownames = FALSE,
  label = "rm1",
  digits.extra = 2
)
```

__Table \ref{em1} contains the average portfolio concentration metrics. Since this market's underlying correlation matrix expresses no dependence between assets, each assets return's are in theory orthogonal. Therefore, the EW portfolio should  produce an ENB of 50, however, the ENB reported is 17.5. This can not simply be due to it being a noisy measure since the ENB reported in Table \ref{em1} is an average across the 1000 ENBs estimated for each portfolio. This miss measurement likely due to a persistent bias that arises then rotating the returns matrix based on noise rather than signal. Since the underlying correlation structure implies that the returns are orthogonal, but finite sample estimates of the correlation matrix will inevitably contain some noise, it is likely that the ENB estimate is finding spurious principle components which is making it an unreliable measure of diversification.__

It is also interesting it see that the ENC and ENB are at odds. Portfolio's rated as highly diversified by the ENC measure are are rated as relatively undiversified by the ENB and _vice verse_. Additionally, the ENC measure coincides with the portfolio risk measures from Table \ref{rm1}, in that those with a high ENC were less volitlie. Meanwhile, those with the most ENB were also the most volatile. This suggests that, when operating in a market where all correlation between assets are estimated to be approximately zero, the ENB can be very misleading and that the ENB is most likely a better tool for measuring concentration. 


```{r market 1 entropy, results='asis'}

load("data/enc_m1.rda")
load("data/enb_m1.rda")
entropy_m1 <- table_entropy(enc = enc_m1, enb = enb_m1)
stargazer::stargazer(
  entropy_m1,
  header = FALSE,
  title = "Market 1 - Portfolio Concentration Metrics",
  type = "latex",
  summary = FALSE,
  rownames = FALSE,
  label = "em1",
  digits.extra = 2
)
```


### Market 2

The portfolios compared here were estimated on the markets simulated using the no clusters correlation matrix (Figure \ref{corr_mats}). Unlike Market 1 the average portfolio risk metrics in Table \ref{rm2} do not indicate a clear winner.  Despite having the highest average SD the MD portfolio achieved the highest Sharp ratio. The ERC performed best in the the SD, DD and VaR measures while obtaining the second highest Sharp ratio. The EW and IV portfolios were similar in their performance and the MV performed poorly. 

Since this market type exhibits significant correlation between its constituents it is unsurprising to observe the EW and IV portfolios fall out, in favor of the more sophisticated ERC and MD portfolios.

The MV portfolio has the second lowest SD, but second highest DD and lowest sharp ratio. This demonstrates its failure to mitigate its downside risk exposure (measured by DD and VaR) which is arguably far more important than reducing volatility in general. Despite the MV having the second lowest SD, it attained the lowest overall Sharp ratio, indicating that it performed exceedingly poorly from a total return perspective. Due to these reasons the MV portfolio is ranked as Market 2's clear looser.

The EW and IV portfolios performed very similarly in Market 2. The equal weight has the higher sharp ratio of the two, while the IV has the performs better when considering SD, DD and VaR. It therefore seems that the EW portfolio finds itself sightly closer to the top right quadrant of the efficient frontier, where it earns higher returns with greater risk compared to the IV portfolio. 

The MD portfolio acts as Market 2's wild card. It has the highest sharp ratio by a significant margin, but performs the worst in the SD an DD risk metrics. Despite it having the largest SD, the MD is tied for the largest VaR. This indicates that that compared to the other portfolios, the MD earned exceptionally high returns. In comparison the ERC seems to be the safer and more consistent option. It scored the best over all across the three risk metrics and achieved the second highest Sharp ratio. Therefore, from a risk mitigation perspective the ERC is the clear winner, however the high returns obtained by the MD portfolio may entice less risk averse investors. 


```{r market 2, results = 'asis'}
source("code/table_risk_metrics.R")
load("data/perf_m2.rda")

risk_metrics_m2 <- table_risk_metrics(perf_m2)
stargazer::stargazer(
  risk_metrics_m2,
  header = FALSE,
  title = "Market 2 - Portfolio Risk Metrics",
  type = "latex",
  summary = FALSE,
  rownames = FALSE,
  label = "rm2",
  digits.extra = 2
)
```

```{r market 2 entropy, eval=FALSE, include=FALSE, results='asis'}

load("data/enc_m2.rda")
load("data/enb_m2.rda")
entropy_m2 <- table_entropy(enc = enc_m2, enb = enb_m2)
stargazer::stargazer(
  entropy_m1,
  header = FALSE,
  title = "Market 2 - Portfolio Concentration Metrics",
  type = "latex",
  summary = FALSE,
  rownames = FALSE,
  label = "em2",
  digits.extra = 2
)
```

### Market 3

The portfolios compared here were estimated on the markets simulated using the five clusters correlation matrix (Figure \ref{corr_mats}). The portfolios average Sharp ratio (Sharp), standard deviation (SD), DD and VaR across the 10 000 simulated markets are shown in Table \ref{rm3}.

Despite there being significant and distinct risk clusters in the  markets 

```{r market 3, results = 'asis'}
source("code/table_risk_metrics.R")
load("data/perf_m3.rda")

risk_metrics_m3 <- table_risk_metrics(perf_m3)
stargazer::stargazer(
  risk_metrics_m3,
  header = FALSE,
  title = "Market 3 Risk Metrics",
  type = "latex",
  summary = FALSE,
  rownames = FALSE,
  label = "rm3",
  digits.extra = 2
)
```


### Market 4

```{r market 4, results = 'asis'}
source("code/table_risk_metrics.R")
load("data/perf_m4.rda")

risk_metrics_m4 <- table_risk_metrics(perf_m4)
stargazer::stargazer(
  risk_metrics_m4,
  header = FALSE,
  title = "Market 4 Risk Metrics",
  type = "latex",
  summary = FALSE,
  rownames = FALSE,
  label = "eigens",
  digits.extra = 2
)
```


### Market 5


```{r market 5, results = 'asis'}
source("code/table_risk_metrics.R")
load("data/perf_m5.rda")

risk_metrics_m5 <- table_risk_metrics(perf_m5)
stargazer::stargazer(
  risk_metrics_m5,
  header = FALSE,
  title = "Market 5 Risk Metrics",
  type = "latex",
  summary = FALSE,
  rownames = FALSE,
  label = "eigens",
  digits.extra = 2
)
```

## Comparing Portfolios Across Market Types

### Naive

### Minimum Variance

### Inverse Volatility

### Equal Risk Contribution

### Maximum Diversification

## Discussion

# Conclusion \label{conclusion}

I hope you find this template useful. Remember, stackoverflow is your friend - use it to find answers to questions. Feel free to write me a mail if you have any questions regarding the use of this package. To cite this package, simply type citation("Texevier") in Rstudio to get the citation for @Texevier (Note that united references in your bibtex file will not be included in References).


<!-- Make title of bibliography here: -->
<!-- \newpage -->

\newpage
# References {-}

<div id="refs"></div>

\newpage
# Appendix {-}
```{r means and sds, results="asis"}
load("data/emp_mu.rda")
load("data/emp_sd.rda")

moments <-
  tibble(
    Asset =  glue::glue("Asset_{1:length(emp_mu)}"),
    Mu = emp_mu %>% as.double() %>% round(5),
    Sd = emp_sd %>% as.double() %>% round(5)
  )
moment_table <-
  bind_cols(moments[1:25, ], moments[26:50, ]) %>% set_names( c("Asset", "Mean", "Sd", "Asset...", "Mean...", "Sd..."))

stargazer::stargazer(
  moment_table,
  header = FALSE,
  title = "Asset Means and Sd's",
  type = "latex",
  summary = FALSE,
  rownames = FALSE,
  label = "msd",
  digits.extra = 2
)



```



### Hierarchical Risk Parity (HRP)

> The maximum drawdown is calculated by first, calculating portfolio cumulative returns and the maximum cumulative return achieved. The maximum drawdown is then the maximum amount that the cumulative return dips below its maximum, it is measured as a percentage of the maximum cumulative return [@PerformanceAnalytics]. 

Due to the multitude of robustness issues related to traditional portfolio optimisers, @lopez developed a new approach incorporating machine-learning methods and graph theory [@arevalo]. @lopez argues that the "lack of hierarchical structure in a correlation matrix allows weights to vary freely in unintended ways" and that this contributes to the instability issues. His HRP algorithm requires only a singular co-variance matrix and can utilize the information within without the need for the positive definite property [@lopez]. This procedure works in three stages:

@lopez carried out an in-sample simulation study comparing the respective allocations of the long-only minimum variance, IVP and HRP portfolios using a co-variance matrix using a condition number that is "not unfavourable" to the minimum variance portfolio. The simulated data consisted of 10000 observations across 10 variables. The following findings were made: The minimum variance portfolio concentrated 92.66% of funds in the top 5 holdings and assigned a zero weight to 3 assets. _Conversly_, HRP only assigned 62.5% of its funds to the top 5 holdings [@lopez]. The minimum variance portfolio's objective function causes it to build highly concentrated portfolio's in favor of a small reduction in volatility; the HRP portfolio had only a slightly higher volatility [@lopez]. This apparent diversification advantage achieved by the minimum variance portfolio is rather deceptive as the portfolio remains highly susceptible to idiosyncratic risk incidents within its top holdings [@lopez]. This claim was further validated by the finding that HRP achieved significantly lower out of sample variance compared to the minimum variance portfolio.


## Appendix A {-}

Some appendix information here

## Appendix B {-}

